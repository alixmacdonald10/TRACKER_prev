{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TRACKER-PreProcessing-Denoising.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO4rKefArgJpnp+1wsSfXvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20a0a9c5dd764e6589c4ecfd0aad3ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e78d403b40d4eed8f008b999ac98191",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d094dc5602b46e4a007756bb11ef0c9",
              "IPY_MODEL_9148417bfe6f40a787d498f53c7721a3",
              "IPY_MODEL_0a38ab98d30849a087e4d7239ba75b54"
            ]
          }
        },
        "9e78d403b40d4eed8f008b999ac98191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d094dc5602b46e4a007756bb11ef0c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_17bb49769f7e438fae7a68e5e27a0e49",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9c94c01757c4f32b7d07fc25efae800"
          }
        },
        "9148417bfe6f40a787d498f53c7721a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dc30d57004f74c259c58435ddd2635c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e68d792024a24227ac92c80985f3afb9"
          }
        },
        "0a38ab98d30849a087e4d7239ba75b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6441afeca63b43679cb039cbcedabd18",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/2 [04:45&lt;04:45, 285.78s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92c67025f87a4ff5a5739206e5dd5019"
          }
        },
        "17bb49769f7e438fae7a68e5e27a0e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9c94c01757c4f32b7d07fc25efae800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc30d57004f74c259c58435ddd2635c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e68d792024a24227ac92c80985f3afb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6441afeca63b43679cb039cbcedabd18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92c67025f87a4ff5a5739206e5dd5019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afad4b4a30fb4f87bf9f4ecd6b239848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cad1533609124471a6e2a74839d7034f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_67bc91694c4942d69ab74fbc2f97adca",
              "IPY_MODEL_d83890b76b4844208b876300feefb9dd",
              "IPY_MODEL_b65fc806942140acaff730829dc2da66"
            ]
          }
        },
        "cad1533609124471a6e2a74839d7034f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67bc91694c4942d69ab74fbc2f97adca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e781fd602f71472395952b48c74bf6fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb0cfde8419b4ef18f1459907037fcf5"
          }
        },
        "d83890b76b4844208b876300feefb9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1c763f17b57a44b09fda270f10c8f71d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 32,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 32,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0567ec5f8e184b618603da1422dcd559"
          }
        },
        "b65fc806942140acaff730829dc2da66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f880c820a6b94d2ba4098d7f2747c1f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32/32 [04:11&lt;00:00,  7.28s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1198c611b4b482980fb19ba103e967e"
          }
        },
        "e781fd602f71472395952b48c74bf6fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb0cfde8419b4ef18f1459907037fcf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c763f17b57a44b09fda270f10c8f71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0567ec5f8e184b618603da1422dcd559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f880c820a6b94d2ba4098d7f2747c1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1198c611b4b482980fb19ba103e967e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58fc833d4ed14a7dbb2f134ce937f2d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_48036cfe768a4182a38029ab141e2b48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9a7883c97487458c96fb016af0b5d076",
              "IPY_MODEL_d1440b67f5864158880e4ed6538b00c3",
              "IPY_MODEL_671a9650f48647c3bd8b4c42e71d04bf"
            ]
          }
        },
        "48036cfe768a4182a38029ab141e2b48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a7883c97487458c96fb016af0b5d076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8dca2eac338e47929d5f959f30eb3165",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35137665931741749087998b87e8f9b2"
          }
        },
        "d1440b67f5864158880e4ed6538b00c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f10c23b9358d4c34862ea98ef93da1fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aec8ca83dc58468b9a9a35dde50f3419"
          }
        },
        "671a9650f48647c3bd8b4c42e71d04bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e8121fdc7f948558cd97460b9109d74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:34&lt;00:00,  7.75s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66b5a8adfaff4b3baa8fedd33608cc51"
          }
        },
        "8dca2eac338e47929d5f959f30eb3165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35137665931741749087998b87e8f9b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f10c23b9358d4c34862ea98ef93da1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aec8ca83dc58468b9a9a35dde50f3419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e8121fdc7f948558cd97460b9109d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66b5a8adfaff4b3baa8fedd33608cc51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c08da4e5d53440739c2885b0308e3b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4a699b4bdf104ca19db61946b8990ea1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_157bd8f9597c48e69a2d2b2385655d28",
              "IPY_MODEL_0187d815fd9944b097fd4ed54d3ad9b5",
              "IPY_MODEL_af9a4dfcf0be4183b3fa6d063ee82028"
            ]
          }
        },
        "4a699b4bdf104ca19db61946b8990ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "157bd8f9597c48e69a2d2b2385655d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10dcdc35ee25423a837744914c22c873",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2918e4209683470db25d6d631fae7cba"
          }
        },
        "0187d815fd9944b097fd4ed54d3ad9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1ad01cb06f1413dacaa479172a1a508",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 32,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 32,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7bc2d8b744b42e18045351be328ca0b"
          }
        },
        "af9a4dfcf0be4183b3fa6d063ee82028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb89eec630a0414e923b293f6e3fa12b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32/32 [04:22&lt;00:00,  7.55s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac272383cf7d42438c09004b9cb75492"
          }
        },
        "10dcdc35ee25423a837744914c22c873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2918e4209683470db25d6d631fae7cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1ad01cb06f1413dacaa479172a1a508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7bc2d8b744b42e18045351be328ca0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb89eec630a0414e923b293f6e3fa12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac272383cf7d42438c09004b9cb75492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5e48d54abb1411184a55bfe55285d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e63126634db442e9c3d562bb4186ecc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1b840c3293604083945446d50fa5807c",
              "IPY_MODEL_b1a160150e5f47b0bbd686efd85468c1",
              "IPY_MODEL_5b6e707baf2f4006839f66ef43662a55"
            ]
          }
        },
        "8e63126634db442e9c3d562bb4186ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b840c3293604083945446d50fa5807c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8542606a55024299a48a06ea9ea10119",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f4ed5cf7a1741dcb7096afe3ddffd0b"
          }
        },
        "b1a160150e5f47b0bbd686efd85468c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3475c6f31e8c4eef802c3c529bf26457",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2f9606e541644b0b0e9b3ad0f5ef662"
          }
        },
        "5b6e707baf2f4006839f66ef43662a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3203fbf1935a4fc89f846b9f961fa72c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/4 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_942efc9d0a6e4647b16fec1e35047003"
          }
        },
        "8542606a55024299a48a06ea9ea10119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f4ed5cf7a1741dcb7096afe3ddffd0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3475c6f31e8c4eef802c3c529bf26457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2f9606e541644b0b0e9b3ad0f5ef662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3203fbf1935a4fc89f846b9f961fa72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "942efc9d0a6e4647b16fec1e35047003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alixmacdonald10/TRACKER/blob/main/TRACKER_PreProcessing_Denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odvung2WoAG7"
      },
      "source": [
        "#PREREQUISITS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvjdqiSYWK_b"
      },
      "source": [
        "##Mount drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKSt4uB5Hyqf",
        "outputId": "c967e44a-6f17-42d2-be80-1e8ac6116480"
      },
      "source": [
        "from google.colab import drive\n",
        " \n",
        "# mount drive to access file\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/your_project_folder/'  #change dir to your project folder"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7wVpTTyoQp_"
      },
      "source": [
        "##Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stNoG62OsN2T",
        "outputId": "ca0fc3bc-5dd5-4ef7-84ee-02057a29e873"
      },
      "source": [
        "# install weights and bias for logging\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamackerel\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD6qFaOcHabQ",
        "outputId": "1925a55c-0d1e-4c28-de77-747f8202e079"
      },
      "source": [
        "# NN imports\n",
        "!pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install tensorboard"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.9.0+cu102 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision==0.10.0+cu102 in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: torchaudio===0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu102) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu102) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu102) (1.19.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.41.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpVmyQVndxRl",
        "outputId": "9d41123c-ca07-4873-f863-6b6b740ac3af"
      },
      "source": [
        "# database specific imports\n",
        "!pip3 install h5py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf4Gre85krt6"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7t7mUj8SA2R"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxDHfKBW50hY"
      },
      "source": [
        "##Check Cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7QlP9Eb7fyt",
        "outputId": "7f6dc048-467d-4ec6-a497-212a3a7e1ece"
      },
      "source": [
        "# Check devices\n",
        "num_devices = torch.cuda.device_count()\n",
        "print(f'Number of cuda devices: {num_devices}')\n",
        "for device in range(0, num_devices):\n",
        "  device_name = torch.cuda.get_device_name(device)\n",
        "  print(f'Cuda device name: {device_name}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cuda devices: 1\n",
            "Cuda device name: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYIWYgu6oemA"
      },
      "source": [
        "#MODEL INFORMATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpgflIdcqHby"
      },
      "source": [
        "##Set hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBGZ7StmqJCZ"
      },
      "source": [
        "# define hyper parameters\n",
        "project_name = 'Tracker-PreProcessing-Denoising'\n",
        "verbose = 1  # print out helpers\n",
        "transformed_when = 'after patches'\n",
        "transform1 = 'hflip'\n",
        "transform2 = 'rotate'\n",
        "patch_size = 256\n",
        "patch_stride = (12 * patch_size)\n",
        "plot_itterations = 1\n",
        "num_epoch = 2\n",
        "max_itterations = 4e5\n",
        "initial_learning_rate = 2e-4\n",
        "min_learning_rate = 1e-7\n",
        "loss_type = \"PSNR\"\n",
        "scheduler_type = \"cosine_annealing\"\n",
        "optimizer_type = \"Adam\"\n",
        "if optimizer_type == \"SGD\":\n",
        "  batch = 1\n",
        "else:\n",
        "  batch = 4  #  max batch size for cuda memory\n",
        "mini_batch_size = 4\n",
        "fpath = '/content/gdrive/MyDrive/Programming/datasets/noisyDataset.hdf5'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGCjD-JAvzW4"
      },
      "source": [
        "# Track hyperparameters and run metadata\n",
        "hyperparameters = {\n",
        "  \"device_name\": device_name,\n",
        "  \"transformed_when\": transformed_when,\n",
        "  \"transform1\": transform1,\n",
        "  \"transform2\": transform2,\n",
        "  \"verbose\": verbose,\n",
        "  \"patch_size\": patch_size,\n",
        "  \"patch_stride\": patch_stride,\n",
        "  \"plot_itterations\": plot_itterations,\n",
        "  \"batch\": batch,\n",
        "  \"mini_batch_size\": mini_batch_size,\n",
        "  \"num_epoch\": num_epoch,\n",
        "  \"max_itterations\": max_itterations,\n",
        "  \"initial_learning_rate\": initial_learning_rate,\n",
        "  \"min_learning_rate\": min_learning_rate,\n",
        "  \"scheduler_type\": scheduler_type,\n",
        "  \"loss_type\": \"PSNR\",\n",
        "  \"optimizer_type\": optimizer_type,\n",
        "  \"architecture\": \"HINet\",\n",
        "  \"dataset\": \"Smartphone Image Denoising Dataset (SIDD)\",\n",
        "  \"project_name\": project_name\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYXbowqpWVMB"
      },
      "source": [
        "##Define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvDOKrIDzX0S"
      },
      "source": [
        "###Note for transfer learning and importing pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CZ94AmjzzWjc",
        "outputId": "7544ae61-a2a2-485c-9ce6-2536966a042c"
      },
      "source": [
        "''' \n",
        "Models can be imported from saved files and transfer learned by:\n",
        " \n",
        "'''\n",
        "#from torchvision import models\n",
        " \n",
        "#model = models.resnet101(pretrained=True)\n",
        " \n",
        "# TO ONLY TRAIN THE LAST LAYER (TRANSFER LEARNING) DO THE FOLLOWING\n",
        "#for param in model.parameters():\n",
        "#   param.requires_grad = False  # freezes all layers in beginning\n",
        " \n",
        "# return number of features in output (to transfer learn)\n",
        "#num_ftrs = model.fc.in_features\n",
        " \n",
        "# set output fully connected using\n",
        "#model.fc = nn.Linear(num_ftrs, 2)   # (input layers, output layers)\n",
        "#model.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\nModels can be imported from saved files and transfer learned by:\\n \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhaJ-ruRpKHQ"
      },
      "source": [
        "###Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu7d0oiR2WcU"
      },
      "source": [
        "'''\n",
        "HINet: Half Instance Normalization Network for Image Restoration\n",
        "@inproceedings{chen2021hinet,\n",
        "  title={HINet: Half Instance Normalization Network for Image Restoration},\n",
        "  author={Liangyu Chen and Xin Lu and Jie Zhang and Xiaojie Chu and Chengpeng Chen},\n",
        "  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},\n",
        "  year={2021}\n",
        "}\n",
        "'''\n",
        "\n",
        "\n",
        "def conv3x3(in_chn, out_chn, bias=True):\n",
        "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "    return layer\n",
        "\n",
        "def conv_down(in_chn, out_chn, bias=False):\n",
        "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "    return layer\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, bias=False, stride = 1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size,\n",
        "        padding=(kernel_size//2), bias=bias, stride = stride)\n",
        "\n",
        "## Supervised Attention Module\n",
        "class SAM(nn.Module):\n",
        "    def __init__(self, n_feat, kernel_size=3, bias=True):\n",
        "        super(SAM, self).__init__()\n",
        "        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n",
        "        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n",
        "        self.conv3 = conv(3, n_feat, kernel_size, bias=bias)\n",
        "\n",
        "    def forward(self, x, x_img):\n",
        "        x1 = self.conv1(x)\n",
        "        img = self.conv2(x) + x_img\n",
        "        x2 = torch.sigmoid(self.conv3(img))\n",
        "        x1 = x1*x2\n",
        "        x1 = x1+x\n",
        "        return x1, img\n",
        "\n",
        "class HINet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_chn=3, wf=64, depth=5, relu_slope=0.2, hin_position_left=0, hin_position_right=4):\n",
        "        super(HINet, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.down_path_1 = nn.ModuleList()\n",
        "        self.down_path_2 = nn.ModuleList()\n",
        "        self.conv_01 = nn.Conv2d(in_chn, wf, 3, 1, 1)\n",
        "        self.conv_02 = nn.Conv2d(in_chn, wf, 3, 1, 1)\n",
        "\n",
        "        prev_channels = self.get_input_chn(wf)\n",
        "        for i in range(depth): #0,1,2,3,4\n",
        "            use_HIN = True if hin_position_left <= i and i <= hin_position_right else False\n",
        "            downsample = True if (i+1) < depth else False\n",
        "            self.down_path_1.append(UNetConvBlock(prev_channels, (2**i) * wf, downsample, relu_slope, use_HIN=use_HIN))\n",
        "            self.down_path_2.append(UNetConvBlock(prev_channels, (2**i) * wf, downsample, relu_slope, use_csff=downsample, use_HIN=use_HIN))\n",
        "            prev_channels = (2**i) * wf\n",
        "\n",
        "        self.up_path_1 = nn.ModuleList()\n",
        "        self.up_path_2 = nn.ModuleList()\n",
        "        self.skip_conv_1 = nn.ModuleList()\n",
        "        self.skip_conv_2 = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path_1.append(UNetUpBlock(prev_channels, (2**i)*wf, relu_slope))\n",
        "            self.up_path_2.append(UNetUpBlock(prev_channels, (2**i)*wf, relu_slope))\n",
        "            self.skip_conv_1.append(nn.Conv2d((2**i)*wf, (2**i)*wf, 3, 1, 1))\n",
        "            self.skip_conv_2.append(nn.Conv2d((2**i)*wf, (2**i)*wf, 3, 1, 1))\n",
        "            prev_channels = (2**i)*wf\n",
        "        self.sam12 = SAM(prev_channels)\n",
        "        self.cat12 = nn.Conv2d(prev_channels*2, prev_channels, 1, 1, 0)\n",
        "\n",
        "        self.last = conv3x3(prev_channels, in_chn, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        image = x\n",
        "        #stage 1\n",
        "        x1 = self.conv_01(image)\n",
        "        encs = []\n",
        "        decs = []\n",
        "        for i, down in enumerate(self.down_path_1):\n",
        "            if (i+1) < self.depth:\n",
        "                x1, x1_up = down(x1)\n",
        "                encs.append(x1_up)\n",
        "            else:\n",
        "                x1 = down(x1)\n",
        "\n",
        "        for i, up in enumerate(self.up_path_1):\n",
        "            x1 = up(x1, self.skip_conv_1[i](encs[-i-1]))\n",
        "            decs.append(x1)\n",
        "\n",
        "        sam_feature, out_1 = self.sam12(x1, image)\n",
        "        #stage 2\n",
        "        x2 = self.conv_02(image)\n",
        "        x2 = self.cat12(torch.cat([x2, sam_feature], dim=1))\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_path_2):\n",
        "            if (i+1) < self.depth:\n",
        "                x2, x2_up = down(x2, encs[i], decs[-i-1])\n",
        "                blocks.append(x2_up)\n",
        "            else:\n",
        "                x2 = down(x2)\n",
        "\n",
        "        for i, up in enumerate(self.up_path_2):\n",
        "            x2 = up(x2, self.skip_conv_2[i](blocks[-i-1]))\n",
        "\n",
        "        out_2 = self.last(x2)\n",
        "        out_2 = out_2 + image\n",
        "        return [out_1, out_2]\n",
        "\n",
        "    def get_input_chn(self, in_chn):\n",
        "        return in_chn\n",
        "\n",
        "    def _initialize(self):\n",
        "        gain = nn.init.calculate_gain('leaky_relu', 0.20)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, gain=gain)\n",
        "                if not m.bias is None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, downsample, relu_slope, use_csff=False, use_HIN=False):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.identity = nn.Conv2d(in_size, out_size, 1, 1, 0)\n",
        "        self.use_csff = use_csff\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1, bias=True)\n",
        "        self.relu_1 = nn.LeakyReLU(relu_slope, inplace=False)\n",
        "        self.conv_2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1, bias=True)\n",
        "        self.relu_2 = nn.LeakyReLU(relu_slope, inplace=False)\n",
        "\n",
        "        if downsample and use_csff:\n",
        "            self.csff_enc = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
        "            self.csff_dec = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
        "\n",
        "        if use_HIN:\n",
        "            self.norm = nn.InstanceNorm2d(out_size//2, affine=True)\n",
        "        self.use_HIN = use_HIN\n",
        "\n",
        "        if downsample:\n",
        "            self.downsample = conv_down(out_size, out_size, bias=False)\n",
        "\n",
        "    def forward(self, x, enc=None, dec=None):\n",
        "        out = self.conv_1(x)\n",
        "\n",
        "        if self.use_HIN:\n",
        "            out_1, out_2 = torch.chunk(out, 2, dim=1)\n",
        "            out = torch.cat([self.norm(out_1), out_2], dim=1)\n",
        "        out = self.relu_1(out)\n",
        "        out = self.relu_2(self.conv_2(out))\n",
        "\n",
        "        out += self.identity(x)\n",
        "        if enc is not None and dec is not None:\n",
        "            assert self.use_csff\n",
        "            out = out + self.csff_enc(enc) + self.csff_dec(dec)\n",
        "        if self.downsample:\n",
        "            out_down = self.downsample(out)\n",
        "            return out_down, out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, relu_slope):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, bias=True)\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, False, relu_slope)\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        out = torch.cat([up, bridge], 1)\n",
        "        out = self.conv_block(out)\n",
        "        return out\n",
        "\n",
        "class Subspace(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(Subspace, self).__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.blocks.append(UNetConvBlock(in_size, out_size, False, 0.2))\n",
        "        self.shortcut = nn.Conv2d(in_size, out_size, kernel_size=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sc = self.shortcut(x)\n",
        "        for i in range(len(self.blocks)):\n",
        "            x = self.blocks[i](x)\n",
        "        return x + sc\n",
        "\n",
        "\n",
        "class skip_blocks(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size, repeat_num=1):\n",
        "        super(skip_blocks, self).__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.re_num = repeat_num\n",
        "        mid_c = 128\n",
        "        self.blocks.append(UNetConvBlock(in_size, mid_c, False, 0.2))\n",
        "        for i in range(self.re_num - 2):\n",
        "            self.blocks.append(UNetConvBlock(mid_c, mid_c, False, 0.2))\n",
        "        self.blocks.append(UNetConvBlock(mid_c, out_size, False, 0.2))\n",
        "        self.shortcut = nn.Conv2d(in_size, out_size, kernel_size=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sc = self.shortcut(x)\n",
        "        for m in self.blocks:\n",
        "            x = m(x)\n",
        "        return x + sc"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oPJq9JApAKP"
      },
      "source": [
        "###Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pF0gR-fWMr"
      },
      "source": [
        "**NOTE:** The dataset section can vary between analysis!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHR6hyVoXJnT"
      },
      "source": [
        "####Dataset class and transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIhPd41Kzbau"
      },
      "source": [
        "# define dataset class\n",
        "\n",
        "class HDF5Dataset(data.Dataset):\n",
        "  \"\"\"Represents a HDF5 dataset. with X = 'data' and y = 'labels'\n",
        "  \n",
        "  Input params:\n",
        "      file_path: Path to the HDF5 file\n",
        "      transform: PyTorch transform to apply to every data instance (default=None).\n",
        "  \"\"\"\n",
        "  def __init__(self, config, file_loc=None, transform=True):\n",
        "    super().__init__()\n",
        "    self.transform = transform\n",
        "    self.config = config\n",
        "\n",
        "    P = Path(file_loc)\n",
        "    # Search for all h5 files in path\n",
        "    self.file = h5py.File(P, 'r')\n",
        "    # return number of samples\n",
        "    self.n_samples = len(self.file.get('data'))\n",
        "    \n",
        "    # create patches of patch_size\n",
        "    self.patch_size = config.patch_size\n",
        "    self.stride = config.patch_stride  # for even patches\n",
        "       \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    '''\n",
        "    returns items X and y. If config\n",
        "    '''\n",
        "    X, y = self.load_file(index)  # -> [w, h, c]\n",
        "\n",
        "    if self.config.transformed_when == 'before':\n",
        "      if self.transform:\n",
        "        X, y = transform_func(X, y, self.config)\n",
        "    else:    \n",
        "      X = torch.from_numpy(np.float32(X)).permute(2, 0, 1)  # -> [c, w, h]\n",
        "      if self.config.patch_size > 0:\n",
        "        X = X.unfold(1, self.patch_size, self.stride).unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size, self.stride) # -> [c, vert_patches, horiz_patches, img, height, width]\n",
        "        X = torch.flatten(X, start_dim=1, end_dim=3).permute(1, 0, 2, 3) / 255 # -> [patches, c, height, width]  0 to 1 \n",
        "\n",
        "      y = torch.from_numpy(np.float32(y)).permute(2, 0, 1)  # -> [c, w, h]\n",
        "      if self.config.patch_size > 0:\n",
        "        y = y.unfold(1, self.patch_size, self.stride).unfold(2, self.patch_size, self.stride).unfold(3, self.patch_size, self.stride) # -> [c, vert_patches, horiz_patches, img, height, width]\n",
        "        y = torch.flatten(y, start_dim=1, end_dim=3).permute(1, 0, 2, 3) / 255 # -> [patches, c, height, width]  0 to 1\n",
        "\n",
        "      num_patches = int(X.shape[0])\n",
        "\n",
        "      if self.transform:\n",
        "        for img in range(0, num_patches):\n",
        "          X_temp, y_temp = transform_func(X[img], y[img], self.config)\n",
        "          X_temp = X_temp[None, :, :]\n",
        "          y_temp = y_temp[None, :, :]\n",
        "          torch.cat((X, X_temp), 0)\n",
        "          torch.cat((y, y_temp), 0)\n",
        "    return (X, y)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    # allows length of dataset to be returned\n",
        "    return self.n_samples\n",
        "\n",
        "\n",
        "  def load_file(self, index):\n",
        "    ''' load index of database '''\n",
        "    X = self.file.get('data')[index]\n",
        "    y = self.file.get('labels')[index]\n",
        "\n",
        "    return (X, y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymOH1g01_cW"
      },
      "source": [
        "class MiniBatchDataset(HDF5Dataset):\n",
        "  \"\"\"Represents a HDF5 dataset. with X = 'data' and y = 'labels'\n",
        "  \n",
        "  Input params:\n",
        "      file_path: Path to the HDF5 file\n",
        "      transform: PyTorch transform to apply to every data instance (default=None).\n",
        "  \"\"\"\n",
        "  def __init__(self, X, y, config):\n",
        "    super().__init__(config, fpath)\n",
        "    assert X.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.y.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlfZyy0GjDTS"
      },
      "source": [
        "  #transform functions\n",
        "  \n",
        "  def transform_func(image, label, config):\n",
        "\n",
        "      '''\n",
        "      Apply transforms to image and label\n",
        "      '''\n",
        "      \n",
        "      # convert to PIL image\n",
        "      image = transforms.functional.to_pil_image(image)\n",
        "      label = transforms.functional.to_pil_image(label)\n",
        "\n",
        "      if config.transform1 == \"hflip\":\n",
        "        # Random horizontal flipping\n",
        "        if random.random() > 0.5:\n",
        "            image = transforms.functional.hflip(image)\n",
        "            label = transforms.functional.hflip(label)\n",
        "\n",
        "      if config.transform1 == \"hflip\":\n",
        "        # Random 90 deg rotation\n",
        "        if random.random() > 0.5:\n",
        "            image = transforms.functional.rotate(image, 90)\n",
        "            label = transforms.functional.rotate(label, 90)\n",
        "\n",
        "      # Transform to tensor\n",
        "      image = transforms.functional.to_tensor(image)\n",
        "      label = transforms.functional.to_tensor(label)\n",
        "\n",
        "      return image, label"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehwgeTtEnkHr"
      },
      "source": [
        "####Split dataset into train, val, test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVIpfN1xDJau"
      },
      "source": [
        "def train_val_test_split(dataset):\n",
        "\n",
        "  # perform random splits on dataset to return train, val, test sets (manual seed fixes output for repeatable results , remove device = cuda for non. GPU )\n",
        "  dataset_length = len(dataset)\n",
        "\n",
        "  # set sizes\n",
        "  train_set_size = 0.8\n",
        "  val_set_size = 0.1\n",
        "  test_set_size = 0.1\n",
        "\n",
        "  # return lengths\n",
        "  train_set_length = round(train_set_size * dataset_length, 0)\n",
        "  val_set_length = round(val_set_size * dataset_length, 0)\n",
        "  test_set_length = dataset_length - train_set_length - val_set_length\n",
        "\n",
        "  # check\n",
        "  total = train_set_length + val_set_length + test_set_length\n",
        "  print(f'total length: {total} / train set length: {train_set_length} / validation set length: {val_set_length} / test set length: {test_set_length}')\n",
        "  assert total == dataset_length\n",
        "\n",
        "  # create datasets\n",
        "  train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "      dataset,\n",
        "      [int(train_set_length), int(val_set_length), int(test_set_length)],\n",
        "      generator=torch.Generator().manual_seed(42)\n",
        "  )\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNw8LDzoXSvH"
      },
      "source": [
        "####Test plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAxKNdk3gpVj"
      },
      "source": [
        "def test_plot_Xy_batch(dataset):\n",
        " \n",
        "  # plot images\n",
        "  num_cols = 4\n",
        "  num_patches = int(num_cols / 2)\n",
        "  num_rows = 2\n",
        "  fig, axs = plt.subplots(num_rows, num_cols, figsize=(16, 16), sharey=True)\n",
        "  fig.suptitle(\"Train sample example images\")\n",
        "\n",
        "  row = 0\n",
        "  for image in range(0, num_rows):  # plot two images in batch\n",
        "    X, y = dataset.__getitem__(image)\n",
        "    col = 0\n",
        "    for patch in range(0, num_patches):  #plot patches\n",
        "      X_img_patches = X.permute(0, 2, 3, 1)\n",
        "      X_img = X_img_patches[patch]\n",
        "\n",
        "      y_img_patches = y.permute(0, 2, 3, 1)\n",
        "      y_img = y_img_patches[patch]\n",
        "\n",
        "      axs[row, col].imshow(X_img)\n",
        "      axs[row, col].title.set_text(f'Batch {image} X train example patch {patch}')\n",
        "      col += 1\n",
        "      axs[row, col].imshow(y_img)\n",
        "      axs[row, col].title.set_text(f'Batch {image} y train example patch {patch}')\n",
        "      col += 1\n",
        "    row += 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GII5d-kId-Fz"
      },
      "source": [
        "def test_plot_Xy_train_val_test_dataset(train_dataset, val_dataset, test_dataset):\n",
        "  # Get dataset items\n",
        "  test_index = 1\n",
        "  X_train, y_train = train_dataset.__getitem__(test_index)\n",
        "  X_val, y_val = val_dataset.__getitem__(test_index)\n",
        "  X_test, y_test = test_dataset.__getitem__(test_index)\n",
        "\n",
        "  patch = 0\n",
        "  \n",
        "  # plot images\n",
        "  fig, axs = plt.subplots(3, 2, figsize=(16, 16), sharey=True)\n",
        "  fig.suptitle(\"Train, validation, test dataset sample example images\")\n",
        "  axs[0, 0].imshow(X_train.permute(0, 2, 3, 1)[patch])\n",
        "  axs[0, 0].title.set_text('X train example image')\n",
        "  axs[0, 1].imshow(y_train.permute(0, 2, 3, 1)[patch])\n",
        "  axs[0, 1].title.set_text('y train example image')\n",
        "  axs[1, 0].imshow(X_val.permute(0, 2, 3, 1)[patch])\n",
        "  axs[1, 0].title.set_text('X validation example image')\n",
        "  axs[1, 1].imshow(y_val.permute(0, 2, 3, 1)[patch])\n",
        "  axs[1, 1].title.set_text('y validation example image')\n",
        "  axs[2, 0].imshow(X_test.permute(0, 2, 3, 1)[patch])\n",
        "  axs[2, 0].title.set_text('X test example image')\n",
        "  axs[2, 1].imshow(y_test.permute(0, 2, 3, 1)[patch])\n",
        "  axs[2, 1].title.set_text('y test example image')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJKW1rUt8bKh"
      },
      "source": [
        "###Custom Loss Function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76lRNVcp330E"
      },
      "source": [
        "# create custom PSNRLoss class\n",
        "\n",
        "class PSNRLoss(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PSNRLoss, self).__init__()\n",
        "\n",
        "  def forward(self, R, X, y, data_range=1.0, reduction='mean', convert_to_greyscale=False):\n",
        "      r\"\"\"Compute Peak Signal-to-Noise Ratio for a batch of images.\n",
        "      Supports both greyscale and color images with RGB channel order.\n",
        "      Args:\n",
        "          R: An input tensor. Shape :math:`(2, N, C, H, W)`.  where position 0 is [stage1 output, stage 2 output] -> outputs from model\n",
        "          x: An input tensor. Shape :math:`(N, C, H, W)`.\n",
        "          y: A target tensor. Shape :math:`(N, C, H, W)`.\n",
        "          data_range: Maximum value range of images (usually 1.0 or 255).\n",
        "          reduction: Specifies the reduction type:\n",
        "              ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``\n",
        "          convert_to_greyscale: Convert RGB image to YCbCr format and computes PSNR\n",
        "              only on luminance channel if `True`. Compute on all 3 channels otherwise.\n",
        "      Returns:\n",
        "          PSNR Index of similarity betwen two images.\n",
        "      References:\n",
        "          https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n",
        "      \"\"\"\n",
        "      # Constant for numerical stability\n",
        "      EPS = 1e-8\n",
        "\n",
        "      X = X / float(data_range)\n",
        "      y = y / float(data_range)\n",
        "      R[0] = (R[0] / float(data_range)).to(device)\n",
        "      R[1] = (R[1] / float(data_range)).to(device)\n",
        "      \n",
        "\n",
        "      if (X.size(1) == 3) and convert_to_greyscale:\n",
        "          # Convert RGB image to YCbCr and take luminance: Y = 0.299 R + 0.587 G + 0.114 B\n",
        "          rgb_to_grey = torch.tensor([0.299, 0.587, 0.114]).view(1, -1, 1, 1).to(X)\n",
        "          R[0] = torch.sum(R[0] * rgb_to_grey, dim=1, keepdim=True)\n",
        "          R[1] = torch.sum(R[1] * rgb_to_grey, dim=1, keepdim=True)\n",
        "          X = torch.sum(X * rgb_to_grey, dim=1, keepdim=True)\n",
        "          y = torch.sum(y * rgb_to_grey, dim=1, keepdim=True)\n",
        "\n",
        "      score = []\n",
        "      for i in range(0, len(R)):\n",
        "        mse = torch.mean(((R[i].add(X)) - y) ** 2, dim=[1, 2, 3])\n",
        "        max_value = 1. if X[0].max() <= 1 else 255. # max pixel value\n",
        "        score.append(20. * torch.log10(max_value / torch.sqrt(mse)))\n",
        "\n",
        "      summed_loss = score[0].add(score[1])\n",
        "      summed_loss = -1. * summed_loss\n",
        "\n",
        "      return _reduced(summed_loss, reduction) # reduced to single value\n",
        "\n",
        "    \n",
        "def _reduced(loss, reduction_type):\n",
        "  r\"\"\"Reduce input in batch dimension if needed.\n",
        "  Args:\n",
        "      x: Tensor with shape (N, *).\n",
        "      reduction: Specifies the reduction type:\n",
        "          ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``\n",
        "  \"\"\"\n",
        "  if reduction_type == 'none':\n",
        "      return loss\n",
        "  elif reduction_type == 'mean':\n",
        "      return loss.mean(dim=0)\n",
        "  elif reduction_type == 'sum':\n",
        "      return loss.sum(dim=0)\n",
        "  else:\n",
        "      raise ValueError(\"Uknown reduction. Expected one of {'none', 'mean', 'sum'}\")\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L0asjYlIyLN"
      },
      "source": [
        "###Make model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGqP3iEGlLKd"
      },
      "source": [
        "def make(config, device):\n",
        "    \n",
        "    verbose = config.verbose\n",
        "\n",
        "    # create dataset\n",
        "    file_loc = fpath\n",
        "    dataset = HDF5Dataset(config, file_loc=fpath, transform=True)\n",
        "    train_dataset, val_dataset, test_dataset = train_val_test_split(dataset)\n",
        "    \n",
        "    if verbose==1:\n",
        "      # plot dataset info\n",
        "      test_plot_Xy_batch(train_dataset)\n",
        "      test_plot_Xy_train_val_test_dataset(train_dataset, val_dataset, test_dataset)\n",
        "\n",
        "    # create dataloader\n",
        "    train_dataloader = make_loader(config, train_dataset)\n",
        "    val_dataloader = make_loader(config, val_dataset)\n",
        "    test_dataloader = make_loader(config, test_dataset)\n",
        "    # set up a dictionary of dataloaders for train and val\n",
        "    dataloader = {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}\n",
        "    dataset_sizes = {'train': len(train_dataloader), 'val': len(val_dataloader), 'test': len(test_dataloader)} \n",
        "    if verbose==1:\n",
        "      print(f'Dataset sizes: {dataset_sizes}')\n",
        "\n",
        "    # Make the model\n",
        "    model = HINet().to(device)\n",
        "\n",
        "    # set loss type\n",
        "    if config.loss_type == \"PSNR\":\n",
        "      criterion = PSNRLoss().to(device)  # custom loss\n",
        "    elif config.loss_type == \"MSE\":\n",
        "      criterion = MSELoss().to(device)\n",
        "\n",
        "    # set optimizer type\n",
        "    if config.optimizer_type == \"Adam\":\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=config.initial_learning_rate)\n",
        "    elif config.optimizer_type == \"SGD\":\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=config.initial_learning_rate)\n",
        "\n",
        "    # set learning rate scheduler \n",
        "    if config.scheduler_type == \"cosine_annealing\":\n",
        "      model_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.max_itterations, eta_min=config.min_learning_rate, last_epoch=-1, verbose=False)\n",
        "\n",
        "\n",
        "    return model, dataloader, dataset_sizes, criterion, optimizer, model_lr_scheduler"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaKVAT3sz16n"
      },
      "source": [
        "####Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akfREZjZQ4Dm"
      },
      "source": [
        "def make_loader(config, dataset):\n",
        "    loader = data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=config.batch, \n",
        "        shuffle=False,\n",
        "        pin_memory=False, \n",
        "        num_workers=1\n",
        "    )\n",
        "    return loader"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IXRPNZWdssT"
      },
      "source": [
        "###Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85bWkHvp0l81"
      },
      "source": [
        "# training loop\n",
        "\n",
        "def train_model(model, config, device, dataloader, dataset_sizes, criterion, optimizer, scheduler):\n",
        "  verbose = config.verbose\n",
        "  plot_itter = config.plot_itterations\n",
        "  num_epochs = config.num_epoch\n",
        "  mini_batch_size = config.mini_batch_size\n",
        "  # tell wandb to watch\n",
        "  wandb.watch(model, criterion, log=\"all\", log_freq=plot_itter)\n",
        "  # start timer\n",
        "  since = time.time()\n",
        "  if verbose == 1:\n",
        "    print('Total progress...')\n",
        "  #initialise variables\n",
        "  output_dict = {\n",
        "      'num_training_examples': 0\n",
        "  }\n",
        "  num_itters = 0\n",
        "  num_training_examples = 0\n",
        "  num_validation_examples = 0\n",
        "  # load existing best weights and reset accuracy\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  # begin analysis\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    since_epoch = time.time()\n",
        "    if num_itters > max_itterations:\n",
        "      break\n",
        "    else:\n",
        "      print(f'\\nEpoch {epoch + 1} / {num_epochs}')\n",
        "      print('-' * 10)\n",
        "\n",
        "      for phase in ['train', 'val']:\n",
        "        since_phase = time.time()\n",
        "        if verbose == 1:\n",
        "          print(f'Phase -> {phase}')\n",
        "          print(f'Batches...')\n",
        "          print('-' * 10)\n",
        "        # loop train and val phase for each dataset\n",
        "        best_loss = 1e5  # large so always saves best loss\n",
        "        epoch_running_loss = 0\n",
        "        epoch_loss = 0\n",
        "        total_imgs_in_batches = 0\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader[phase])):\n",
        "          # get images and labels\n",
        "          images_batch = torch.flatten(images, start_dim=0, end_dim=1)  # -> [imgs (batch*patches), channels, height, width]\n",
        "          labels_batch = torch.flatten(labels, start_dim=0, end_dim=1)       \n",
        "          imgs_in_batch = images_batch.shape[0]\n",
        "          total_imgs_in_batches += imgs_in_batch\n",
        "\n",
        "          # create dataset for mini-batches\n",
        "          mini_batch_data = MiniBatchDataset(images_batch, labels_batch, config) \n",
        "          mini_batch_dataloader = make_loader(config, mini_batch_data) \n",
        "          if phase == 'train':\n",
        "            print(f'Mini-batches -> Total training examples = {num_training_examples}')\n",
        "\n",
        "          for mini_batch_idx, (mini_batch_images, mini_batch_labels) in enumerate(mini_batch_dataloader):\n",
        "            # get images labels and indices\n",
        "            mini_batch_images = mini_batch_images.to(device)  # -> [mb, c, h, w]\n",
        "            mini_batch_labels = mini_batch_labels.to(device)\n",
        "            imgs_in_mini_batch = mini_batch_images.shape[0]\n",
        "\n",
        "            if phase == 'train':  # TRAINING\n",
        "              model.train().to(device)  # set to training mode\n",
        "              # FORWARD PASS\n",
        "              with torch.set_grad_enabled(True):\n",
        "                outputs = model(mini_batch_images) # -> [mini_batch, channels, height, width] -> outputs is [out_1 (STAGE 1 output image), out_2 (STAGE 2 output image)]\n",
        "                loss = criterion(outputs, mini_batch_images, mini_batch_labels, data_range=1.0, reduction='mean').to(device)\n",
        "                num_training_examples += mini_batch_images.shape[0]\n",
        "                epoch_running_loss += (loss.item() * imgs_in_mini_batch)  # mult by batch to allow avg (epoch loss) to be calcd.\n",
        "                # BACKWARD PASS\n",
        "                optimizer.zero_grad()  # emptys cache\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                wandb.log(\n",
        "                  {\"learning rate\": scheduler.get_last_lr()},\n",
        "                  step=num_training_examples,\n",
        "                  commit=False\n",
        "                )\n",
        "                num_itters += 1\n",
        "\n",
        "            else:  # VALIDATION\n",
        "              model.eval()  # set to training mode\n",
        "              # FORWARD PASS\n",
        "              with torch.set_grad_enabled(False):\n",
        "                outputs = model(mini_batch_images) # -> [mini_batch, channels, height, width] -> outputs is [out_1 (STAGE 1 output image), out_2 (STAGE 2 output image)]\n",
        "                loss = criterion(outputs, mini_batch_images, mini_batch_labels, data_range=1.0, reduction='mean').to(device)\n",
        "                num_validation_examples += mini_batch_images.shape[0]\n",
        "                epoch_running_loss += (loss.item() * imgs_in_mini_batch)\n",
        "                num_itters += 1\n",
        "          \n",
        "          if [plot_itter==1 and batch_idx==0 and mini_batch_idx==0 and phase=='val'] or [epoch+1%plot_itter==0 and batch_idx==0 and mini_batch_idx==0 and phase=='val']:  # every Xth val epoch save data from 0th batch and 0th minibatch for plotting\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"input image\": [wandb.Image(mini_batch_images[0].permute(1, 2, 0).to('cpu').numpy(), caption=\"Input image\")],\n",
        "                \"ground truth image\": [wandb.Image(mini_batch_labels[0].permute(1, 2, 0).to('cpu').numpy(), caption=\"Ground truth image\")],\n",
        "                \"Output image stage 1\": [wandb.Image(outputs[0][0].permute(1, 2, 0).to('cpu').detach().numpy(), caption=\"Output image stage 1\")],\n",
        "                \"Output image stage 2\": [wandb.Image(outputs[1][0].permute(1, 2, 0).to('cpu').detach().numpy(), caption=\"Output image stage 2\")]\n",
        "                },\n",
        "                step=num_training_examples,\n",
        "                commit=False\n",
        "            )\n",
        "        \n",
        "        # print phase time\n",
        "        phase_time = time.time() - since_phase \n",
        "        print(f'Epoch {epoch+1} {phase} time: {phase_time // 60:.0f}m {phase_time % 60:.0f}s')\n",
        "        \n",
        "        # log epoch loss\n",
        "        epoch_loss = epoch_running_loss / total_imgs_in_batches # average of epoch losses\n",
        "        print(f'Epoch {epoch+1} {phase} loss: {epoch_loss}')\n",
        "        if epoch % plot_itter == 0 or plot_itter==1:  # every Xth epoch save data for plotting\n",
        "            wandb.log({f\"epoch {phase} loss\": epoch_loss}, step=num_training_examples)\n",
        "\n",
        "        # deep copy the model if best loss\n",
        "        if phase == 'val' and epoch_loss < best_loss: \n",
        "          best_loss = epoch_loss\n",
        "          best_loss_epoch = epoch\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        del loss, outputs\n",
        "\n",
        "    # print time for epoch\n",
        "    epoch_time = time.time() - since_epoch\n",
        "    print(f'Epoch {epoch+1} time: {epoch_time // 60:.0f}m {epoch_time % 60:.0f}s')\n",
        "    print(f'Current best validation loss: {best_loss:.4f} @ epoch {best_loss_epoch+1}')\n",
        "\n",
        "  # print total training time over all epochs\n",
        "  total_time = time.time() - since\n",
        "  print(f'\\nTotal time for training {num_epochs} epochs: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n",
        "  output_dict['num_training_examples'] = num_training_examples\n",
        "\n",
        "  return output_dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmeWK6FRJin4"
      },
      "source": [
        "####Training loop image plotting check function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNgwb6oTLLUK"
      },
      "source": [
        "# # write function to plot figure of input v output to tensorboard\n",
        "# def plot_output_comparison(output_dict, batch_size):\n",
        "#     '''\n",
        "#     Generates matplotlib Figure using a trained network, along with images\n",
        "#     from a batch\n",
        "#     '''\n",
        "#     # output list shape -> [(epoch, item, batch)]\n",
        "#     columns = ['noisy', 'ground truth']\n",
        " \n",
        "#     # append epochs to noisy and ground truth list - e.g. noisy, ground truth, epoch 1, epoch 2, ...\n",
        "#     [columns.append(f'Epoch {epoch+1}') for epoch in output_dict['epoch']]\n",
        " \n",
        "#     # plot the images from X input (noisy), y output (GT) and acorss the epochs for the entire batch\n",
        "#     fig, axs = plt.subplots(batch, len(columns), figsize=(16, 16), sharey=True)\n",
        "#     fig.suptitle(\"Model Output\")\n",
        "#     for i in range(0, batch):  # rows\n",
        "#       epoch_number = 0 \n",
        "#       for j in range(0, len(columns)):  # columns\n",
        "#         if i == 0:\n",
        "#           # create column labels\n",
        "#           axs[i, j].set_title(f'{columns[j]}')\n",
        "#         if j == 0:\n",
        "#           # create row labels\n",
        "#           axs[i, j].set_ylabel(f'Image\\nbatch\\n{i+1}', rotation=0, size='large', labelpad=50)\n",
        " \n",
        "#         # plot all epochs of image batch set\n",
        "#         if j == 0:\n",
        "#           X_img = normalize_output(output_dict['images'][0][i]) # -> [dictionary][image types][batch]\n",
        "#           axs[i, j].imshow(X_img.permute(1, 2, 0).detach().numpy())  \n",
        "#           axs[i, j].set_xticks([])\n",
        "#           axs[i, j].set_yticks([])\n",
        "#         elif j == 1:\n",
        "#           y_img = normalize_output(output_dict['labels'][0][i])\n",
        "#           axs[i, j].imshow(y_img.permute(1, 2, 0).detach().numpy())\n",
        "#           axs[i, j].set_xticks([])\n",
        "#           axs[i, j].set_yticks([])\n",
        "#         else:\n",
        "#           output_img = normalize_output(output_dict['outputs'][j-2][1])\n",
        "#           print(f'outputimg shape: output_img.shape')\n",
        "#           axs[i, j].imshow( output_img.permute(1, 2, 0).detach().numpy())\n",
        "#           axs[i, j].set_xticks([])\n",
        "#           axs[i, j].set_yticks([])\n",
        "#           epoch_number += 1\n",
        " \n",
        "#     fig.subplots_adjust(hspace=0, wspace=0)\n",
        "\n",
        " \n",
        "#     return fig"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yaur8LWUHPya"
      },
      "source": [
        "def normalize_output(img):\n",
        "    img = img - img.min()\n",
        "    img = img / img.max()\n",
        "    return img"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQvfNO3JmT-"
      },
      "source": [
        "###Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHiALgWUnrUT"
      },
      "source": [
        "def test(model, config, device, criterion, dataloader, dataset_sizes, num_training_examples):\n",
        "  if config.verbose == 1:\n",
        "    print(f'Phase -> testing')\n",
        "    print('-' * 10)\n",
        "    \n",
        "  with torch.no_grad(): # stops computation of grads\n",
        "    running_loss = 0\n",
        "    total_imgs = 0\n",
        "    num_test_examples = 0\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(dataloader['test'])):\n",
        "      # get images and labels\n",
        "      images_batch = torch.flatten(images, start_dim=0, end_dim=1)  # -> [imgs (batch*patches), channels, height, width]\n",
        "      labels_batch = torch.flatten(labels, start_dim=0, end_dim=1)       \n",
        "      imgs_in_batch = images_batch.shape[0]\n",
        "      total_imgs += imgs_in_batch\n",
        "\n",
        "      # create dataset for mini-batches\n",
        "      mini_batch_data = MiniBatchDataset(images_batch, labels_batch, config) \n",
        "      mini_batch_dataloader = make_loader(config, mini_batch_data) \n",
        "      print(f'Mini-batches -> Testing after {num_training_examples} -> Test examples = {num_test_examples}')\n",
        "      for mini_batch_idx, (mini_batch_images, mini_batch_labels) in enumerate(mini_batch_dataloader):\n",
        "        # get images labels and indices\n",
        "        mini_batch_images = mini_batch_images.to(device)  # -> [mb, c, h, w]\n",
        "        mini_batch_labels = mini_batch_labels.to(device)\n",
        "        mini_batch_size = mini_batch_images.shape[0]\n",
        "\n",
        "        outputs = model(mini_batch_images) # -> [mini_batch, channels, height, width] -> outputs is [out_1 (STAGE 1 output image), out_2 (STAGE 2 output image)]\n",
        "        output_stage1 = outputs[0].to(device)\n",
        "        output_stage2 = outputs[1].to(device)\n",
        "        # determine losses    \n",
        "        loss = criterion(outputs, mini_batch_images, mini_batch_labels, data_range=1.0, reduction='mean').to(device)\n",
        "        num_test_examples += mini_batch_images.shape[0]\n",
        "        running_loss += loss.item() * mini_batch_size  # mult by batch to allow avg (epoch loss) to be calcd.\n",
        "      \n",
        "\n",
        "    total_loss = running_loss / total_imgs\n",
        "    wandb.log({\"Test loss\": total_loss}, step=num_training_examples)\n",
        "    print(f'\\nTest loss of the network: {loss}')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VwPW9_3JvNH"
      },
      "source": [
        "###Model pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTby5tpBGHqE"
      },
      "source": [
        "#Model Pipeline - train, val, test\n",
        "\n",
        "def model_pipeline(hyperparameters):\n",
        "    \n",
        "    verbose = hyperparameters['verbose']\n",
        "\n",
        "    # run on device\n",
        "    if torch.cuda.is_available():\n",
        "      device = 'cuda'\n",
        "    else:\n",
        "      device = 'cpu'\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=project_name, config=hyperparameters):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "      \n",
        "      # make the model, data, and optimization problem\n",
        "      model, dataloader, dataset_sizes, criterion, optimizer, model_lr_scheduler = make(config, device)\n",
        "      print(model)\n",
        "\n",
        "      # perform training and evaluating\n",
        "      output_dict = train_model(model, config, device, dataloader, dataset_sizes, criterion, optimizer, model_lr_scheduler)\n",
        "\n",
        "      # Test final performance\n",
        "      test(model, config, device, criterion, dataloader, dataset_sizes, output_dict['num_training_examples'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cWiKtrGgetG"
      },
      "source": [
        "#RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuvrXcY-J00L"
      },
      "source": [
        "##Train, Validate, test!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_TU0rKwJzvm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "20a0a9c5dd764e6589c4ecfd0aad3ff6",
            "9e78d403b40d4eed8f008b999ac98191",
            "7d094dc5602b46e4a007756bb11ef0c9",
            "9148417bfe6f40a787d498f53c7721a3",
            "0a38ab98d30849a087e4d7239ba75b54",
            "17bb49769f7e438fae7a68e5e27a0e49",
            "d9c94c01757c4f32b7d07fc25efae800",
            "dc30d57004f74c259c58435ddd2635c5",
            "e68d792024a24227ac92c80985f3afb9",
            "6441afeca63b43679cb039cbcedabd18",
            "92c67025f87a4ff5a5739206e5dd5019",
            "afad4b4a30fb4f87bf9f4ecd6b239848",
            "cad1533609124471a6e2a74839d7034f",
            "67bc91694c4942d69ab74fbc2f97adca",
            "d83890b76b4844208b876300feefb9dd",
            "b65fc806942140acaff730829dc2da66",
            "e781fd602f71472395952b48c74bf6fa",
            "bb0cfde8419b4ef18f1459907037fcf5",
            "1c763f17b57a44b09fda270f10c8f71d",
            "0567ec5f8e184b618603da1422dcd559",
            "f880c820a6b94d2ba4098d7f2747c1f4",
            "b1198c611b4b482980fb19ba103e967e",
            "58fc833d4ed14a7dbb2f134ce937f2d7",
            "48036cfe768a4182a38029ab141e2b48",
            "9a7883c97487458c96fb016af0b5d076",
            "d1440b67f5864158880e4ed6538b00c3",
            "671a9650f48647c3bd8b4c42e71d04bf",
            "8dca2eac338e47929d5f959f30eb3165",
            "35137665931741749087998b87e8f9b2",
            "f10c23b9358d4c34862ea98ef93da1fd",
            "aec8ca83dc58468b9a9a35dde50f3419",
            "6e8121fdc7f948558cd97460b9109d74",
            "66b5a8adfaff4b3baa8fedd33608cc51",
            "c08da4e5d53440739c2885b0308e3b96",
            "4a699b4bdf104ca19db61946b8990ea1",
            "157bd8f9597c48e69a2d2b2385655d28",
            "0187d815fd9944b097fd4ed54d3ad9b5",
            "af9a4dfcf0be4183b3fa6d063ee82028",
            "10dcdc35ee25423a837744914c22c873",
            "2918e4209683470db25d6d631fae7cba",
            "f1ad01cb06f1413dacaa479172a1a508",
            "e7bc2d8b744b42e18045351be328ca0b",
            "bb89eec630a0414e923b293f6e3fa12b",
            "ac272383cf7d42438c09004b9cb75492",
            "d5e48d54abb1411184a55bfe55285d3b",
            "8e63126634db442e9c3d562bb4186ecc",
            "1b840c3293604083945446d50fa5807c",
            "b1a160150e5f47b0bbd686efd85468c1",
            "5b6e707baf2f4006839f66ef43662a55",
            "8542606a55024299a48a06ea9ea10119",
            "5f4ed5cf7a1741dcb7096afe3ddffd0b",
            "3475c6f31e8c4eef802c3c529bf26457",
            "f2f9606e541644b0b0e9b3ad0f5ef662",
            "3203fbf1935a4fc89f846b9f961fa72c",
            "942efc9d0a6e4647b16fec1e35047003"
          ]
        },
        "outputId": "72c26410-59c6-4bd6-e301-32e76df64992"
      },
      "source": [
        "model = model_pipeline(hyperparameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/amackerel/Tracker-PreProcessing-Denoising/runs/2kn0gb3h\" target=\"_blank\">kind-night-284</a></strong> to <a href=\"https://wandb.ai/amackerel/Tracker-PreProcessing-Denoising\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total length: 160.0 / train set length: 128.0 / validation set length: 16.0 / test set length: 16.0\n",
            "Dataset sizes: {'train': 32, 'val': 4, 'test': 4}\n",
            "HINet(\n",
            "  (down_path_1): ModuleList(\n",
            "    (0): UNetConvBlock(\n",
            "      (identity): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (1): UNetConvBlock(\n",
            "      (identity): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (2): UNetConvBlock(\n",
            "      (identity): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (3): UNetConvBlock(\n",
            "      (identity): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (4): UNetConvBlock(\n",
            "      (identity): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    )\n",
            "  )\n",
            "  (down_path_2): ModuleList(\n",
            "    (0): UNetConvBlock(\n",
            "      (identity): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (csff_enc): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (csff_dec): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (1): UNetConvBlock(\n",
            "      (identity): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (csff_enc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (csff_dec): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (2): UNetConvBlock(\n",
            "      (identity): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (csff_enc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (csff_dec): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (3): UNetConvBlock(\n",
            "      (identity): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (csff_enc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (csff_dec): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (downsample): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    )\n",
            "    (4): UNetConvBlock(\n",
            "      (identity): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (conv_1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "      (conv_2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    )\n",
            "  )\n",
            "  (conv_01): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv_02): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (up_path_1): ModuleList(\n",
            "    (0): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "    (1): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "    (2): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "    (3): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (up_path_2): ModuleList(\n",
            "    (0): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "    (1): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "    (2): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "    (3): UNetUpBlock(\n",
            "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (conv_block): UNetConvBlock(\n",
            "        (identity): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_1): LeakyReLU(negative_slope=0.2)\n",
            "        (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (relu_2): LeakyReLU(negative_slope=0.2)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (skip_conv_1): ModuleList(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (skip_conv_2): ModuleList(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (sam12): SAM(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (cat12): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n",
            "Total progress...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20a0a9c5dd764e6589c4ecfd0aad3ff6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 2\n",
            "----------\n",
            "Phase -> train\n",
            "Batches...\n",
            "----------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afad4b4a30fb4f87bf9f4ecd6b239848",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini-batches -> Total training examples = 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini-batches -> Total training examples = 8\n",
            "Mini-batches -> Total training examples = 16\n",
            "Mini-batches -> Total training examples = 24\n",
            "Mini-batches -> Total training examples = 32\n",
            "Mini-batches -> Total training examples = 40\n",
            "Mini-batches -> Total training examples = 48\n",
            "Mini-batches -> Total training examples = 56\n",
            "Mini-batches -> Total training examples = 64\n",
            "Mini-batches -> Total training examples = 72\n",
            "Mini-batches -> Total training examples = 80\n",
            "Mini-batches -> Total training examples = 88\n",
            "Mini-batches -> Total training examples = 96\n",
            "Mini-batches -> Total training examples = 104\n",
            "Mini-batches -> Total training examples = 112\n",
            "Mini-batches -> Total training examples = 120\n",
            "Mini-batches -> Total training examples = 128\n",
            "Mini-batches -> Total training examples = 136\n",
            "Mini-batches -> Total training examples = 144\n",
            "Mini-batches -> Total training examples = 152\n",
            "Mini-batches -> Total training examples = 160\n",
            "Mini-batches -> Total training examples = 168\n",
            "Mini-batches -> Total training examples = 176\n",
            "Mini-batches -> Total training examples = 184\n",
            "Mini-batches -> Total training examples = 192\n",
            "Mini-batches -> Total training examples = 200\n",
            "Mini-batches -> Total training examples = 208\n",
            "Mini-batches -> Total training examples = 216\n",
            "Mini-batches -> Total training examples = 224\n",
            "Mini-batches -> Total training examples = 232\n",
            "Mini-batches -> Total training examples = 240\n",
            "Mini-batches -> Total training examples = 248\n",
            "Epoch 1 train time: 4m 11s\n",
            "epoch 1 train loss: -42.06067273020744\n",
            "Phase -> val\n",
            "Batches...\n",
            "----------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58fc833d4ed14a7dbb2f134ce937f2d7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 val time: 0m 35s\n",
            "epoch 1 val loss: -48.30217504501343\n",
            "Epoch 1 time: 4m 46s\n",
            "Epoch best validation loss: -48.3022 @ epoch 1\n",
            "\n",
            "Epoch 2 / 2\n",
            "----------\n",
            "Phase -> train\n",
            "Batches...\n",
            "----------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c08da4e5d53440739c2885b0308e3b96",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini-batches -> Total training examples = 256\n",
            "Mini-batches -> Total training examples = 264\n",
            "Mini-batches -> Total training examples = 272\n",
            "Mini-batches -> Total training examples = 280\n",
            "Mini-batches -> Total training examples = 288\n",
            "Mini-batches -> Total training examples = 296\n",
            "Mini-batches -> Total training examples = 304\n",
            "Mini-batches -> Total training examples = 312\n",
            "Mini-batches -> Total training examples = 320\n",
            "Mini-batches -> Total training examples = 328\n",
            "Mini-batches -> Total training examples = 336\n",
            "Mini-batches -> Total training examples = 344\n",
            "Mini-batches -> Total training examples = 352\n",
            "Mini-batches -> Total training examples = 360\n",
            "Mini-batches -> Total training examples = 368\n",
            "Mini-batches -> Total training examples = 376\n",
            "Mini-batches -> Total training examples = 384\n",
            "Mini-batches -> Total training examples = 392\n",
            "Mini-batches -> Total training examples = 400\n",
            "Mini-batches -> Total training examples = 408\n",
            "Mini-batches -> Total training examples = 416\n",
            "Mini-batches -> Total training examples = 424\n",
            "Mini-batches -> Total training examples = 432\n",
            "Mini-batches -> Total training examples = 440\n",
            "Mini-batches -> Total training examples = 448\n",
            "Mini-batches -> Total training examples = 456\n",
            "Mini-batches -> Total training examples = 464\n",
            "Mini-batches -> Total training examples = 472\n",
            "Mini-batches -> Total training examples = 480\n",
            "Mini-batches -> Total training examples = 488\n",
            "Mini-batches -> Total training examples = 496\n",
            "Mini-batches -> Total training examples = 504\n",
            "Epoch 2 train time: 4m 23s\n",
            "epoch 2 train loss: -51.81391853094101\n",
            "Phase -> val\n",
            "Batches...\n",
            "----------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5e48d54abb1411184a55bfe55285d3b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRpkQ1VMpXoH"
      },
      "source": [
        "##Save model weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pxmCY4kpaHd"
      },
      "source": [
        "# save model state dict\n",
        "PATH = f'/content/gdrive/MyDrive/Programming/models/{project_name}.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIped6zW7XD9"
      },
      "source": [
        "#INFERENCE\n",
        "\n",
        "Run inference using already trained weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaA48NGQ7aBr"
      },
      "source": [
        "# load model weights\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6wcq_y92Y2Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}